{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_BUILD_PARAMS = {\n",
    "    'height': 31,\n",
    "    'width': 200,\n",
    "    'color': False,\n",
    "    'filters': (64, 128, 256, 256, 512, 512, 512),\n",
    "    'rnn_units': (128, 128),\n",
    "    'dropout': 0.25,\n",
    "    'rnn_steps_to_discard': 2,\n",
    "    'pool_size': 2,\n",
    "    'stn': True,\n",
    "}\n",
    "\n",
    "DEFAULT_ALPHABET = string.digits + string.ascii_lowercase\n",
    "\n",
    "PRETRAINED_WEIGHTS = {\n",
    "    'kurapan': {\n",
    "        'alphabet': DEFAULT_ALPHABET,\n",
    "        'build_params': DEFAULT_BUILD_PARAMS,\n",
    "        'weights': {\n",
    "            'notop': {\n",
    "                'url':\n",
    "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan_notop.h5',\n",
    "                'filename': 'crnn_kurapan_notop.h5',\n",
    "                'sha256': '027fd2cced3cbea0c4f5894bb8e9e85bac04f11daf96b8fdcf1e4ee95dcf51b9'\n",
    "            },\n",
    "            'top': {\n",
    "                'url':\n",
    "                'https://github.com/faustomorales/keras-ocr/releases/download/v0.8.4/crnn_kurapan.h5',\n",
    "                'filename': 'crnn_kurapan.h5',\n",
    "                'sha256': 'a7d8086ac8f5c3d6a0a828f7d6fbabcaf815415dd125c32533013f85603be46d'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x, beta=1):\n",
    "    return x * keras.backend.sigmoid(beta * x)\n",
    "\n",
    "\n",
    "keras.utils.get_custom_objects().update({'swish': keras.layers.Activation(swish)})\n",
    "\n",
    "\n",
    "def _repeat(x, num_repeats):\n",
    "    ones = tf.ones((1, num_repeats), dtype='int32')\n",
    "    x = tf.reshape(x, shape=(-1, 1))\n",
    "    x = tf.matmul(x, ones)\n",
    "    return tf.reshape(x, [-1])\n",
    "\n",
    "\n",
    "def _meshgrid(height, width):\n",
    "    x_linspace = tf.linspace(-1., 1., width)\n",
    "    y_linspace = tf.linspace(-1., 1., height)\n",
    "    x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n",
    "    x_coordinates = tf.reshape(x_coordinates, shape=(1, -1))\n",
    "    y_coordinates = tf.reshape(y_coordinates, shape=(1, -1))\n",
    "    ones = tf.ones_like(x_coordinates)\n",
    "    indices_grid = tf.concat([x_coordinates, y_coordinates, ones], 0)\n",
    "    return indices_grid\n",
    "\n",
    "\n",
    "# pylint: disable=too-many-statements\n",
    "def _transform(inputs):\n",
    "    locnet_x, locnet_y = inputs\n",
    "    output_size = locnet_x.shape[1:]\n",
    "    batch_size = tf.shape(locnet_x)[0]\n",
    "    height = tf.shape(locnet_x)[1]\n",
    "    width = tf.shape(locnet_x)[2]\n",
    "    num_channels = tf.shape(locnet_x)[3]\n",
    "\n",
    "    locnet_y = tf.reshape(locnet_y, shape=(batch_size, 2, 3))\n",
    "\n",
    "    locnet_y = tf.reshape(locnet_y, (-1, 2, 3))\n",
    "    locnet_y = tf.cast(locnet_y, 'float32')\n",
    "\n",
    "    output_height = output_size[0]\n",
    "    output_width = output_size[1]\n",
    "    indices_grid = _meshgrid(output_height, output_width)\n",
    "    indices_grid = tf.expand_dims(indices_grid, 0)\n",
    "    indices_grid = tf.reshape(indices_grid, [-1])  # flatten?\n",
    "    indices_grid = tf.tile(indices_grid, tf.stack([batch_size]))\n",
    "    indices_grid = tf.reshape(indices_grid, tf.stack([batch_size, 3, -1]))\n",
    "\n",
    "    transformed_grid = tf.matmul(locnet_y, indices_grid)\n",
    "    x_s = tf.slice(transformed_grid, [0, 0, 0], [-1, 1, -1])\n",
    "    y_s = tf.slice(transformed_grid, [0, 1, 0], [-1, 1, -1])\n",
    "    x = tf.reshape(x_s, [-1])\n",
    "    y = tf.reshape(y_s, [-1])\n",
    "\n",
    "    # Interpolate\n",
    "    height_float = tf.cast(height, dtype='float32')\n",
    "    width_float = tf.cast(width, dtype='float32')\n",
    "\n",
    "    output_height = output_size[0]\n",
    "    output_width = output_size[1]\n",
    "\n",
    "    x = tf.cast(x, dtype='float32')\n",
    "    y = tf.cast(y, dtype='float32')\n",
    "    x = .5 * (x + 1.0) * width_float\n",
    "    y = .5 * (y + 1.0) * height_float\n",
    "\n",
    "    x0 = tf.cast(tf.floor(x), 'int32')\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.cast(tf.floor(y), 'int32')\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    max_y = tf.cast(height - 1, dtype='int32')\n",
    "    max_x = tf.cast(width - 1, dtype='int32')\n",
    "    zero = tf.zeros([], dtype='int32')\n",
    "\n",
    "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "\n",
    "    flat_image_dimensions = width * height\n",
    "    pixels_batch = tf.range(batch_size) * flat_image_dimensions\n",
    "    flat_output_dimensions = output_height * output_width\n",
    "    base = _repeat(pixels_batch, flat_output_dimensions)\n",
    "    base_y0 = base + y0 * width\n",
    "    base_y1 = base + y1 * width\n",
    "    indices_a = base_y0 + x0\n",
    "    indices_b = base_y1 + x0\n",
    "    indices_c = base_y0 + x1\n",
    "    indices_d = base_y1 + x1\n",
    "\n",
    "    flat_image = tf.reshape(locnet_x, shape=(-1, num_channels))\n",
    "    flat_image = tf.cast(flat_image, dtype='float32')\n",
    "    pixel_values_a = tf.gather(flat_image, indices_a)\n",
    "    pixel_values_b = tf.gather(flat_image, indices_b)\n",
    "    pixel_values_c = tf.gather(flat_image, indices_c)\n",
    "    pixel_values_d = tf.gather(flat_image, indices_d)\n",
    "\n",
    "    x0 = tf.cast(x0, 'float32')\n",
    "    x1 = tf.cast(x1, 'float32')\n",
    "    y0 = tf.cast(y0, 'float32')\n",
    "    y1 = tf.cast(y1, 'float32')\n",
    "\n",
    "    area_a = tf.expand_dims(((x1 - x) * (y1 - y)), 1)\n",
    "    area_b = tf.expand_dims(((x1 - x) * (y - y0)), 1)\n",
    "    area_c = tf.expand_dims(((x - x0) * (y1 - y)), 1)\n",
    "    area_d = tf.expand_dims(((x - x0) * (y - y0)), 1)\n",
    "    transformed_image = tf.add_n([\n",
    "        area_a * pixel_values_a, area_b * pixel_values_b, area_c * pixel_values_c,\n",
    "        area_d * pixel_values_d\n",
    "    ])\n",
    "    # Finished interpolation\n",
    "\n",
    "    transformed_image = tf.reshape(transformed_image,\n",
    "                                   shape=(batch_size, output_height, output_width, num_channels))\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTCDecoder():\n",
    "    def decoder(y_pred):\n",
    "        input_shape = tf.keras.backend.shape(y_pred)\n",
    "        input_length = tf.ones(shape=input_shape[0]) * tf.keras.backend.cast(\n",
    "            input_shape[1], 'float32')\n",
    "        unpadded = tf.keras.backend.ctc_decode(y_pred, input_length)[0][0]\n",
    "        unpadded_shape = tf.keras.backend.shape(unpadded)\n",
    "        padded = tf.pad(unpadded,\n",
    "                        paddings=[[0, 0], [0, input_shape[1] - unpadded_shape[1]]],\n",
    "                        constant_values=-1)\n",
    "        return padded\n",
    "\n",
    "    return tf.keras.layers.Lambda(decoder, name='decode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(alphabet,\n",
    "                height,\n",
    "                width,\n",
    "                color,\n",
    "                filters,\n",
    "                rnn_units,\n",
    "                dropout,\n",
    "                rnn_steps_to_discard,\n",
    "                pool_size,\n",
    "                stn=True):\n",
    "    \"\"\"Build a Keras CRNN model for character recognition.\n",
    "    Args:\n",
    "        height: The height of cropped images\n",
    "        width: The width of cropped images\n",
    "        color: Whether the inputs should be in color (RGB)\n",
    "        filters: The number of filters to use for each of the 7 convolutional layers\n",
    "        rnn_units: The number of units for each of the RNN layers\n",
    "        dropout: The dropout to use for the final layer\n",
    "        rnn_steps_to_discard: The number of initial RNN steps to discard\n",
    "        pool_size: The size of the pooling steps\n",
    "        stn: Whether to add a Spatial Transformer layer\n",
    "    \"\"\"\n",
    "    assert len(filters) == 7, '7 CNN filters must be provided.'\n",
    "    assert len(rnn_units) == 2, '2 RNN filters must be provided.'\n",
    "    inputs = keras.layers.Input((height, width, 3 if color else 1), name='input', batch_size=1)\n",
    "    x = keras.layers.Permute((2, 1, 3))(inputs)\n",
    "    x = keras.layers.Lambda(lambda x: x[:, :, ::-1])(x)\n",
    "    x = keras.layers.Conv2D(filters[0], (3, 3), activation='relu', padding='same', name='conv_1')(x)\n",
    "    x = keras.layers.Conv2D(filters[1], (3, 3), activation='relu', padding='same', name='conv_2')(x)\n",
    "    x = keras.layers.Conv2D(filters[2], (3, 3), activation='relu', padding='same', name='conv_3')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_3')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_3')(x)\n",
    "    x = keras.layers.Conv2D(filters[3], (3, 3), activation='relu', padding='same', name='conv_4')(x)\n",
    "    x = keras.layers.Conv2D(filters[4], (3, 3), activation='relu', padding='same', name='conv_5')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_5')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), name='maxpool_5')(x)\n",
    "    x = keras.layers.Conv2D(filters[5], (3, 3), activation='relu', padding='same', name='conv_6')(x)\n",
    "    x = keras.layers.Conv2D(filters[6], (3, 3), activation='relu', padding='same', name='conv_7')(x)\n",
    "    x = keras.layers.BatchNormalization(name='bn_7')(x)\n",
    "    if stn:\n",
    "        # pylint: disable=pointless-string-statement\n",
    "        \"\"\"Spatial Transformer Layer\n",
    "        Implements a spatial transformer layer as described in [1]_.\n",
    "        Borrowed from [2]_:\n",
    "        downsample_fator : float\n",
    "            A value of 1 will keep the orignal size of the image.\n",
    "            Values larger than 1 will down sample the image. Values below 1 will\n",
    "            upsample the image.\n",
    "            example image: height= 100, width = 200\n",
    "            downsample_factor = 2\n",
    "            output image will then be 50, 100\n",
    "        References\n",
    "        ----------\n",
    "        .. [1]  Spatial Transformer Networks\n",
    "                Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n",
    "                Submitted on 5 Jun 2015\n",
    "        .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n",
    "        .. [3]  https://github.com/EderSantana/seya/blob/keras1/seya/layers/attention.py\n",
    "        \"\"\"\n",
    "        stn_input_output_shape = (width // pool_size**2, height // pool_size**2, filters[6])\n",
    "        stn_input_layer = keras.layers.Input(shape=stn_input_output_shape)\n",
    "        locnet_y = keras.layers.Conv2D(16, (5, 5), padding='same',\n",
    "                                       activation='relu')(stn_input_layer)\n",
    "        locnet_y = keras.layers.Conv2D(32, (5, 5), padding='same', activation='relu')(locnet_y)\n",
    "        locnet_y = keras.layers.Flatten()(locnet_y)\n",
    "        locnet_y = keras.layers.Dense(64, activation='relu')(locnet_y)\n",
    "        locnet_y = keras.layers.Dense(6,\n",
    "                                      weights=[\n",
    "                                          np.zeros((64, 6), dtype='float32'),\n",
    "                                          np.float32([[1, 0, 0], [0, 1, 0]]).flatten()\n",
    "                                      ])(locnet_y)\n",
    "        localization_net = keras.models.Model(inputs=stn_input_layer, outputs=locnet_y)\n",
    "        x = keras.layers.Lambda(_transform,\n",
    "                                output_shape=stn_input_output_shape)([x, localization_net(x)])\n",
    "    x = keras.layers.Reshape(target_shape=(width // pool_size**2,\n",
    "                                           (height // pool_size**2) * filters[-1]),\n",
    "                             name='reshape')(x)\n",
    "\n",
    "    x = keras.layers.Dense(rnn_units[0], activation='relu', name='fc_9')(x)\n",
    "\n",
    "    rnn_1_forward = keras.layers.LSTM(rnn_units[0],\n",
    "                                      kernel_initializer=\"he_normal\",\n",
    "                                      return_sequences=True,\n",
    "                                      name='lstm_10')(x)\n",
    "    rnn_1_back = keras.layers.LSTM(rnn_units[0],\n",
    "                                   kernel_initializer=\"he_normal\",\n",
    "                                   go_backwards=True,\n",
    "                                   return_sequences=True,\n",
    "                                   name='lstm_10_back')(x)\n",
    "    rnn_1_add = keras.layers.Add()([rnn_1_forward, rnn_1_back])\n",
    "    rnn_2_forward = keras.layers.LSTM(rnn_units[1],\n",
    "                                      kernel_initializer=\"he_normal\",\n",
    "                                      return_sequences=True,\n",
    "                                      name='lstm_11')(rnn_1_add)\n",
    "    rnn_2_back = keras.layers.LSTM(rnn_units[1],\n",
    "                                   kernel_initializer=\"he_normal\",\n",
    "                                   go_backwards=True,\n",
    "                                   return_sequences=True,\n",
    "                                   name='lstm_11_back')(rnn_1_add)\n",
    "    x = keras.layers.Concatenate()([rnn_2_forward, rnn_2_back])\n",
    "    backbone = keras.models.Model(inputs=inputs, outputs=x)\n",
    "    x = keras.layers.Dropout(dropout, name='dropout')(x)\n",
    "    x = keras.layers.Dense(len(alphabet) + 1,\n",
    "                           kernel_initializer='he_normal',\n",
    "                           activation='softmax',\n",
    "                           name='fc_12')(x)\n",
    "    x = keras.layers.Lambda(lambda x: x[:, rnn_steps_to_discard:])(x)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=x)\n",
    "    prediction_model = keras.models.Model(inputs=inputs, outputs=CTCDecoder()(model.output))\n",
    "    return model, prediction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_params = DEFAULT_BUILD_PARAMS\n",
    "alphabets = DEFAULT_ALPHABET\n",
    "blank_index = len(alphabets)\n",
    "\n",
    "model, prediction_model = build_model(alphabet=alphabets, **build_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_cache_dir():\n",
    "    return os.environ.get('KERAS_OCR_CACHE_DIR', os.path.expanduser(os.path.join('~',\n",
    "                                                                              '.keras-ocr')))\n",
    "def sha256sum(filename):\n",
    "    \"\"\"Compute the sha256 hash for a file.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    b = bytearray(128 * 1024)\n",
    "    mv = memoryview(b)\n",
    "    with open(filename, 'rb', buffering=0) as f:\n",
    "        for n in iter(lambda: f.readinto(mv), 0):\n",
    "            h.update(mv[:n])\n",
    "    return h.hexdigest()\n",
    "\n",
    "def download_and_verify(url, sha256=None, cache_dir=None, verbose=True, filename=None):\n",
    "    \"\"\"Download a file to a cache directory and verify it with a sha256\n",
    "    hash.\n",
    "    Args:\n",
    "        url: The file to download\n",
    "        sha256: The sha256 hash to check. If the file already exists and the hash\n",
    "            matches, we don't download it again.\n",
    "        cache_dir: The directory in which to cache the file. The default is\n",
    "            `~/.keras-ocr`.\n",
    "        verbose: Whether to log progress\n",
    "        filename: The filename to use for the file. By default, the filename is\n",
    "            derived from the URL.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = get_default_cache_dir()\n",
    "    if filename is None:\n",
    "        filename = os.path.basename(urllib.parse.urlparse(url).path)\n",
    "    filepath = os.path.join(cache_dir, filename)\n",
    "    os.makedirs(os.path.split(filepath)[0], exist_ok=True)\n",
    "    if verbose:\n",
    "        print('Looking for ' + filepath)\n",
    "    if not os.path.isfile(filepath) or (sha256 and sha256sum(filepath) != sha256):\n",
    "        if verbose:\n",
    "            print('Downloading ' + filepath)\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "    assert sha256 is None or sha256 == sha256sum(filepath), 'Error occurred verifying sha256.'\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -r  /home/bhagyarsh/.keras-ocr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for /home/bhagyarsh/.keras-ocr/crnn_kurapan.h5\n",
      "Downloading /home/bhagyarsh/.keras-ocr/crnn_kurapan.h5\n"
     ]
    }
   ],
   "source": [
    "weights_dict = PRETRAINED_WEIGHTS['kurapan']\n",
    "\n",
    "model.load_weights(download_and_verify(url=weights_dict['weights']['top']['url'],\n",
    "                                       filename=weights_dict['weights']['top']['filename'],\n",
    "                                       sha256=weights_dict['weights']['top']['sha256']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(1, 31, 200, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (1, 200, 31, 1)      0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (1, 200, 31, 1)      0           permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (1, 200, 31, 64)     640         lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (1, 200, 31, 128)    73856       conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (1, 200, 31, 256)    295168      conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn_3 (BatchNormalization)       (1, 200, 31, 256)    1024        conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_3 (MaxPooling2D)        (1, 100, 15, 256)    0           bn_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (1, 100, 15, 256)    590080      maxpool_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (1, 100, 15, 512)    1180160     conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn_5 (BatchNormalization)       (1, 100, 15, 512)    2048        conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maxpool_5 (MaxPooling2D)        (1, 50, 7, 512)      0           bn_5[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (1, 50, 7, 512)      2359808     maxpool_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (1, 50, 7, 512)      2359808     conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn_7 (BatchNormalization)       (1, 50, 7, 512)      2048        conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 6)            934902      bn_7[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (1, 50, 7, 512)      0           bn_7[0][0]                       \n",
      "                                                                 model[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (1, 50, 3584)        0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc_9 (Dense)                    (1, 50, 128)         458880      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (1, 50, 128)         131584      fc_9[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10_back (LSTM)             (1, 50, 128)         131584      fc_9[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (1, 50, 128)         0           lstm_10[0][0]                    \n",
      "                                                                 lstm_10_back[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (1, 50, 128)         131584      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11_back (LSTM)             (1, 50, 128)         131584      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (1, 50, 256)         0           lstm_11[0][0]                    \n",
      "                                                                 lstm_11_back[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (1, 50, 256)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "fc_12 (Dense)                   (1, 50, 37)          9509        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (1, 48, 37)          0           fc_12[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 8,794,267\n",
      "Trainable params: 8,791,707\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/represent_data/'\n",
    "def representative_data_gen():\n",
    "    for file in os.listdir(dataset_path):\n",
    "        image_path = dataset_path + file\n",
    "        input_data = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        input_data = cv2.resize(input_data, (200, 31))\n",
    "        input_data = input_data[np.newaxis]\n",
    "        input_data = np.expand_dims(input_data, 3)\n",
    "        input_data = input_data.astype('float32')/255\n",
    "        yield [input_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tflite(quantization):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(prediction_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "    if quantization == 'float16':\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    elif quantization == 'int8' or quantization == 'full_int8':\n",
    "        converter.representative_dataset = representative_data_gen\n",
    "    if quantization == 'full_int8':\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "        converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    tf_lite_model = converter.convert()\n",
    "    open(f'ocr_{quantization}.tflite', 'wb').write(tf_lite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8a2i0esb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8a2i0esb/assets\n"
     ]
    }
   ],
   "source": [
    "quantization = 'dr' #@param [\"dr\", \"float16\"]\n",
    "convert_tflite(quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8emvaf9s/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8emvaf9s/assets\n"
     ]
    }
   ],
   "source": [
    "quantization = 'float16' #@param [\"dr\", \"float16\"] \n",
    "convert_tflite(quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17M\tocr_float16.tflite\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh ocr_float16.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tflite_model(image_path, quantization):\n",
    "    input_data = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    input_data = cv2.resize(input_data, (200, 31))\n",
    "    input_data = input_data[np.newaxis]\n",
    "    input_data = np.expand_dims(input_data, 3)\n",
    "    input_data = input_data.astype('float32')/255\n",
    "    path = 'ocr_float16.tflite'\n",
    "    current = os.path.abspath(os.getcwd())\n",
    "    print(current)\n",
    "    interpreter = tf.lite.Interpreter(model_path=path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_shape = input_details[0]['shape']\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 41, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "image_path = 'represent_data/word_40.png'\n",
    "image = cv2.imread(image_path)\n",
    "window_name = 'image'\n",
    "\n",
    "print(image.shape)\n",
    "#Import image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "#Show the image with matplotlib\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bhagyarsh/Desktop/ocr\n",
      "[[14 10 28 29 14 27 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "tflite_output = run_tflite_model(image_path, 'float16')\n",
    "print(tflite_output)\n",
    "\n",
    "print(len(tflite_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "a\n",
      "s\n",
      "t\n",
      "e\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "for i in tflite_output[0]:\n",
    "    if i != -1:\n",
    "        print(alphabets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_tflite_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d035b3a80e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtflite_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_tflite_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'float16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_tflite_model' is not defined"
     ]
    }
   ],
   "source": [
    "tflite_output = run_tflite_model(image_path, 'float16')\n",
    "print(tflite_output)\n",
    "\n",
    "print(len(tflite_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
